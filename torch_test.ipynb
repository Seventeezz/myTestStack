{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import sys \n",
    "# sys.path.append(r\"E:\\Pystack\\PyStack\\src\")\n",
    "\n",
    "from Game.card_tools import card_tools\n",
    "from NeuralNetwork.value_nn_torch import ValueNn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_huber_loss(y_true, y_pred, delta=1.0):\n",
    "    return F.huber_loss(y_pred, y_true, delta=delta)\n",
    "\n",
    "def masked_huber_loss(y_true, y_pred, delta=1.0):\n",
    "    mask = (y_true != 0).float()\n",
    "    num_active = mask.sum()\n",
    "    base_loss = F.huber_loss(y_pred, y_true, delta=delta, reduction='none')\n",
    "    masked_loss = base_loss * mask\n",
    "    multiplier = y_true.numel() / (num_active + 1e-6)  # 防止除0\n",
    "    return masked_loss.mean() * multiplier\n",
    "\n",
    "# 定义数据Dataset\n",
    "class PokerDataset(Dataset):\n",
    "    def __init__(self, npy_dir):\n",
    "        self.input_paths = sorted([os.path.join(npy_dir, f) for f in os.listdir(npy_dir) if f.startswith(\"inputs\")])\n",
    "        self.target_paths = sorted([os.path.join(npy_dir, f) for f in os.listdir(npy_dir) if f.startswith(\"targets\")])\n",
    "        self.board_paths = sorted([os.path.join(npy_dir, f) for f in os.listdir(npy_dir) if f.startswith(\"boards\")])\n",
    "        \n",
    "        assert len(self.input_paths) == len(self.target_paths) == len(self.board_paths)\n",
    "\n",
    "        # 预加载所有数据\n",
    "        self.inputs = [np.load(p) for p in self.input_paths]\n",
    "        self.targets = [np.load(p) for p in self.target_paths]\n",
    "        self.boards = [np.load(p) for p in self.board_paths]\n",
    "\n",
    "        self.data = []  # (x, y, board) 的扁平化列表\n",
    "        for x_batch, y_batch, board_batch in zip(self.inputs, self.targets, self.boards):\n",
    "            # 计算每个board对应的x_batch重复次数，保证对齐\n",
    "            batch_size = len(x_batch) // len(board_batch)\n",
    "            # board 扩展成和 x_batch 对齐的形状\n",
    "            extended_boards = np.repeat(board_batch, batch_size, axis=0)  # [batch_size * num_boards, board_size]\n",
    "\n",
    "            assert len(x_batch) == len(y_batch) == len(extended_boards), \\\n",
    "                f\"Data length mismatch: {len(x_batch)}, {len(y_batch)}, {len(extended_boards)}\"\n",
    "\n",
    "            for i in range(len(x_batch)):\n",
    "                self.data.append((x_batch[i], y_batch[i], extended_boards[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, board = self.data[idx]\n",
    "        # 1. 使用和TFRecordsConverter完全一致的board转特征方法\n",
    "        b = card_tools.convert_board_to_nn_feature(board)\n",
    "        # 2. 拼接 x 和 b（board的特征）\n",
    "        nn_input = np.zeros(len(x) + len(b), dtype=np.float32)\n",
    "        nn_input[:len(x)] = x\n",
    "        nn_input[len(x):] = b\n",
    "        # 3. mask处理targets，参考TFRecordsConverter中的逻辑\n",
    "        ranges = x[:-1]  # 忽略最后一个pot的值\n",
    "        mask = np.ones_like(ranges, dtype=np.float32)\n",
    "        mask[ranges == 0] = 0\n",
    "        nn_target = y * mask\n",
    "        return torch.tensor(nn_input, dtype=torch.float32), torch.tensor(nn_target, dtype=torch.float32)\n",
    "\n",
    "    def convert_board_to_nn_feature(self, board):\n",
    "        # 这里直接返回float32类型board特征，如果你有card_tools里更复杂的转换，可以替换此处\n",
    "        return board.astype(np.float32)\n",
    "\n",
    "# === 训练函数 ===\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(model.device)\n",
    "        targets = targets.to(model.device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = masked_huber_loss(targets, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# === 验证函数 ===\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(model.device)\n",
    "            targets = targets.to(model.device)\n",
    "            outputs = model(inputs)\n",
    "            loss = masked_huber_loss(targets, outputs)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'n_epochs': 1000,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-4,\n",
    "    'n_workers': 0,\n",
    "    'model_save_path': 'model_best.pt',\n",
    "    \"data_path\": r\".\\data\\TrainSamples\\turn\\root_nodes_npy\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Initializing ValueNn for street 2 | torch version: 2.7.0+cu118\n",
      "Using device: cuda\n",
      "Using randomly initialized weights...\n",
      "ValueNn(\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=2722, out_features=500, bias=True)\n",
      "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): PReLU(num_parameters=1)\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (9): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): PReLU(num_parameters=1)\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "    (12): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (13): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): PReLU(num_parameters=1)\n",
      "    (15): Dropout(p=0.1, inplace=False)\n",
      "    (16): Linear(in_features=500, out_features=2652, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = PokerDataset(CFG['data_path'])\n",
    "val_size = int(0.1 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=CFG['batch_size'], shuffle=True, num_workers=CFG['n_workers'])\n",
    "val_loader = DataLoader(val_set, batch_size=CFG['batch_size'], shuffle=False, num_workers=CFG['n_workers'])\n",
    "\n",
    "# === 初始化模型 ===\n",
    "model = ValueNn(\n",
    "    street=2,                        # 替换成你训练的 street 值（0,1,2,3）\n",
    "    pretrained_weights=False,\n",
    "    approximate='root_nodes'\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1000] Train Loss: 1.309692 | Val Loss: 1.711317\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 2/1000] Train Loss: 1.468755 | Val Loss: 1.712047\n",
      "[Epoch 3/1000] Train Loss: 1.406606 | Val Loss: 1.710885\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 4/1000] Train Loss: 1.303660 | Val Loss: 1.707221\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 5/1000] Train Loss: 1.304313 | Val Loss: 1.687687\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 6/1000] Train Loss: 1.246265 | Val Loss: 1.698994\n",
      "[Epoch 7/1000] Train Loss: 1.500208 | Val Loss: 1.696369\n",
      "[Epoch 8/1000] Train Loss: 1.252023 | Val Loss: 1.700742\n",
      "[Epoch 9/1000] Train Loss: 1.231459 | Val Loss: 1.700673\n",
      "[Epoch 10/1000] Train Loss: 1.343540 | Val Loss: 1.695432\n",
      "[Epoch 11/1000] Train Loss: 1.295765 | Val Loss: 1.673558\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 12/1000] Train Loss: 1.288213 | Val Loss: 1.665544\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 13/1000] Train Loss: 1.568728 | Val Loss: 1.661419\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 14/1000] Train Loss: 1.370239 | Val Loss: 1.660832\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 15/1000] Train Loss: 1.291486 | Val Loss: 1.678404\n",
      "[Epoch 16/1000] Train Loss: 1.256287 | Val Loss: 1.682174\n",
      "[Epoch 17/1000] Train Loss: 1.403252 | Val Loss: 1.666679\n",
      "[Epoch 18/1000] Train Loss: 1.404818 | Val Loss: 1.680618\n",
      "[Epoch 19/1000] Train Loss: 1.289542 | Val Loss: 1.684706\n",
      "[Epoch 20/1000] Train Loss: 1.284977 | Val Loss: 1.663846\n",
      "[Epoch 21/1000] Train Loss: 1.302063 | Val Loss: 1.657055\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 22/1000] Train Loss: 1.550167 | Val Loss: 1.678621\n",
      "[Epoch 23/1000] Train Loss: 1.295089 | Val Loss: 1.683881\n",
      "[Epoch 24/1000] Train Loss: 1.265607 | Val Loss: 1.685705\n",
      "[Epoch 25/1000] Train Loss: 1.260546 | Val Loss: 1.674478\n",
      "[Epoch 26/1000] Train Loss: 1.516283 | Val Loss: 1.679969\n",
      "[Epoch 27/1000] Train Loss: 1.291662 | Val Loss: 1.675248\n",
      "[Epoch 28/1000] Train Loss: 1.224500 | Val Loss: 1.667737\n",
      "[Epoch 29/1000] Train Loss: 1.440847 | Val Loss: 1.676466\n",
      "[Epoch 30/1000] Train Loss: 1.356447 | Val Loss: 1.677159\n",
      "[Epoch 31/1000] Train Loss: 1.304093 | Val Loss: 1.677348\n",
      "[Epoch 32/1000] Train Loss: 1.418877 | Val Loss: 1.666234\n",
      "[Epoch 33/1000] Train Loss: 1.438200 | Val Loss: 1.647341\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 34/1000] Train Loss: 1.275922 | Val Loss: 1.662425\n",
      "[Epoch 35/1000] Train Loss: 1.336018 | Val Loss: 1.667753\n",
      "[Epoch 36/1000] Train Loss: 1.372782 | Val Loss: 1.650706\n",
      "[Epoch 37/1000] Train Loss: 1.254724 | Val Loss: 1.654571\n",
      "[Epoch 38/1000] Train Loss: 1.198494 | Val Loss: 1.653347\n",
      "[Epoch 39/1000] Train Loss: 1.321187 | Val Loss: 1.665243\n",
      "[Epoch 40/1000] Train Loss: 1.233725 | Val Loss: 1.668543\n",
      "[Epoch 41/1000] Train Loss: 1.356745 | Val Loss: 1.670773\n",
      "[Epoch 42/1000] Train Loss: 1.236666 | Val Loss: 1.667547\n",
      "[Epoch 43/1000] Train Loss: 1.336727 | Val Loss: 1.665392\n",
      "[Epoch 44/1000] Train Loss: 1.242872 | Val Loss: 1.666109\n",
      "[Epoch 45/1000] Train Loss: 1.455989 | Val Loss: 1.665558\n",
      "[Epoch 46/1000] Train Loss: 1.330366 | Val Loss: 1.666850\n",
      "[Epoch 47/1000] Train Loss: 1.235475 | Val Loss: 1.665367\n",
      "[Epoch 48/1000] Train Loss: 1.294196 | Val Loss: 1.665329\n",
      "[Epoch 49/1000] Train Loss: 1.221179 | Val Loss: 1.660405\n",
      "[Epoch 50/1000] Train Loss: 1.259047 | Val Loss: 1.660631\n",
      "[Epoch 51/1000] Train Loss: 1.589642 | Val Loss: 1.659505\n",
      "[Epoch 52/1000] Train Loss: 1.239210 | Val Loss: 1.658790\n",
      "[Epoch 53/1000] Train Loss: 1.359047 | Val Loss: 1.629674\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 54/1000] Train Loss: 1.247515 | Val Loss: 1.642891\n",
      "[Epoch 55/1000] Train Loss: 1.353673 | Val Loss: 1.631241\n",
      "[Epoch 56/1000] Train Loss: 1.416583 | Val Loss: 1.617783\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 57/1000] Train Loss: 1.246126 | Val Loss: 1.639833\n",
      "[Epoch 58/1000] Train Loss: 1.221157 | Val Loss: 1.621048\n",
      "[Epoch 59/1000] Train Loss: 1.222753 | Val Loss: 1.627926\n",
      "[Epoch 60/1000] Train Loss: 1.233414 | Val Loss: 1.628991\n",
      "[Epoch 61/1000] Train Loss: 1.412023 | Val Loss: 1.639208\n",
      "[Epoch 62/1000] Train Loss: 1.191216 | Val Loss: 1.632906\n",
      "[Epoch 63/1000] Train Loss: 1.204903 | Val Loss: 1.639200\n",
      "[Epoch 64/1000] Train Loss: 1.256073 | Val Loss: 1.617750\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 65/1000] Train Loss: 1.184930 | Val Loss: 1.622169\n",
      "[Epoch 66/1000] Train Loss: 1.237730 | Val Loss: 1.633033\n",
      "[Epoch 67/1000] Train Loss: 1.481565 | Val Loss: 1.629984\n",
      "[Epoch 68/1000] Train Loss: 1.383323 | Val Loss: 1.628358\n",
      "[Epoch 69/1000] Train Loss: 1.336689 | Val Loss: 1.628003\n",
      "[Epoch 70/1000] Train Loss: 1.264792 | Val Loss: 1.611335\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 71/1000] Train Loss: 1.305762 | Val Loss: 1.624568\n",
      "[Epoch 72/1000] Train Loss: 1.304103 | Val Loss: 1.629109\n",
      "[Epoch 73/1000] Train Loss: 1.469038 | Val Loss: 1.629798\n",
      "[Epoch 74/1000] Train Loss: 1.242376 | Val Loss: 1.610828\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 75/1000] Train Loss: 1.280133 | Val Loss: 1.604092\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 76/1000] Train Loss: 1.174337 | Val Loss: 1.620118\n",
      "[Epoch 77/1000] Train Loss: 1.265004 | Val Loss: 1.635970\n",
      "[Epoch 78/1000] Train Loss: 1.207877 | Val Loss: 1.635938\n",
      "[Epoch 79/1000] Train Loss: 1.204117 | Val Loss: 1.627458\n",
      "[Epoch 80/1000] Train Loss: 1.220599 | Val Loss: 1.632959\n",
      "[Epoch 81/1000] Train Loss: 1.199847 | Val Loss: 1.630906\n",
      "[Epoch 82/1000] Train Loss: 1.432398 | Val Loss: 1.625479\n",
      "[Epoch 83/1000] Train Loss: 1.196629 | Val Loss: 1.619462\n",
      "[Epoch 84/1000] Train Loss: 1.543249 | Val Loss: 1.602595\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 85/1000] Train Loss: 1.253967 | Val Loss: 1.617870\n",
      "[Epoch 86/1000] Train Loss: 1.222898 | Val Loss: 1.608172\n",
      "[Epoch 87/1000] Train Loss: 1.176754 | Val Loss: 1.623070\n",
      "[Epoch 88/1000] Train Loss: 1.177602 | Val Loss: 1.602459\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 89/1000] Train Loss: 1.174035 | Val Loss: 1.594445\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 90/1000] Train Loss: 1.173864 | Val Loss: 1.608483\n",
      "[Epoch 91/1000] Train Loss: 1.259033 | Val Loss: 1.596244\n",
      "[Epoch 92/1000] Train Loss: 1.341112 | Val Loss: 1.608310\n",
      "[Epoch 93/1000] Train Loss: 1.309491 | Val Loss: 1.622610\n",
      "[Epoch 94/1000] Train Loss: 1.379828 | Val Loss: 1.617708\n",
      "[Epoch 95/1000] Train Loss: 1.743104 | Val Loss: 1.627951\n",
      "[Epoch 96/1000] Train Loss: 1.305647 | Val Loss: 1.623589\n",
      "[Epoch 97/1000] Train Loss: 1.257570 | Val Loss: 1.627851\n",
      "[Epoch 98/1000] Train Loss: 1.452007 | Val Loss: 1.619843\n",
      "[Epoch 99/1000] Train Loss: 1.298534 | Val Loss: 1.625149\n",
      "[Epoch 100/1000] Train Loss: 1.443252 | Val Loss: 1.608624\n",
      "[Epoch 101/1000] Train Loss: 2.148999 | Val Loss: 1.624080\n",
      "[Epoch 102/1000] Train Loss: 1.221820 | Val Loss: 1.618781\n",
      "[Epoch 103/1000] Train Loss: 2.216664 | Val Loss: 1.623882\n",
      "[Epoch 104/1000] Train Loss: 1.238181 | Val Loss: 1.599163\n",
      "[Epoch 105/1000] Train Loss: 1.259192 | Val Loss: 1.593798\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 106/1000] Train Loss: 1.340367 | Val Loss: 1.589085\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 107/1000] Train Loss: 1.248024 | Val Loss: 1.611036\n",
      "[Epoch 108/1000] Train Loss: 1.386969 | Val Loss: 1.594646\n",
      "[Epoch 109/1000] Train Loss: 1.516081 | Val Loss: 1.589939\n",
      "[Epoch 110/1000] Train Loss: 1.200331 | Val Loss: 1.600447\n",
      "[Epoch 111/1000] Train Loss: 1.261875 | Val Loss: 1.587461\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 112/1000] Train Loss: 1.212057 | Val Loss: 1.594574\n",
      "[Epoch 113/1000] Train Loss: 1.522368 | Val Loss: 1.602698\n",
      "[Epoch 114/1000] Train Loss: 1.252669 | Val Loss: 1.613134\n",
      "[Epoch 115/1000] Train Loss: 2.032545 | Val Loss: 1.612333\n",
      "[Epoch 116/1000] Train Loss: 1.189746 | Val Loss: 1.604288\n",
      "[Epoch 117/1000] Train Loss: 1.238308 | Val Loss: 1.606776\n",
      "[Epoch 118/1000] Train Loss: 1.421319 | Val Loss: 1.612466\n",
      "[Epoch 119/1000] Train Loss: 1.169553 | Val Loss: 1.585936\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 120/1000] Train Loss: 1.369774 | Val Loss: 1.581506\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 121/1000] Train Loss: 1.518369 | Val Loss: 1.603173\n",
      "[Epoch 122/1000] Train Loss: 1.232813 | Val Loss: 1.608887\n",
      "[Epoch 123/1000] Train Loss: 1.286827 | Val Loss: 1.607208\n",
      "[Epoch 124/1000] Train Loss: 1.223418 | Val Loss: 1.605416\n",
      "[Epoch 125/1000] Train Loss: 1.353091 | Val Loss: 1.608697\n",
      "[Epoch 126/1000] Train Loss: 1.189284 | Val Loss: 1.583735\n",
      "[Epoch 127/1000] Train Loss: 1.253366 | Val Loss: 1.581180\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 128/1000] Train Loss: 1.277886 | Val Loss: 1.599846\n",
      "[Epoch 129/1000] Train Loss: 1.259764 | Val Loss: 1.601732\n",
      "[Epoch 130/1000] Train Loss: 1.356403 | Val Loss: 1.601807\n",
      "[Epoch 131/1000] Train Loss: 1.222400 | Val Loss: 1.580269\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 132/1000] Train Loss: 1.204344 | Val Loss: 1.590585\n",
      "[Epoch 133/1000] Train Loss: 1.506561 | Val Loss: 1.578704\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 134/1000] Train Loss: 1.179629 | Val Loss: 1.599228\n",
      "[Epoch 135/1000] Train Loss: 1.208289 | Val Loss: 1.598204\n",
      "[Epoch 136/1000] Train Loss: 1.211888 | Val Loss: 1.608410\n",
      "[Epoch 137/1000] Train Loss: 1.237763 | Val Loss: 1.592999\n",
      "[Epoch 138/1000] Train Loss: 1.228381 | Val Loss: 1.588152\n",
      "[Epoch 139/1000] Train Loss: 1.283331 | Val Loss: 1.575880\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 140/1000] Train Loss: 1.242668 | Val Loss: 1.599042\n",
      "[Epoch 141/1000] Train Loss: 1.287380 | Val Loss: 1.606641\n",
      "[Epoch 142/1000] Train Loss: 1.267475 | Val Loss: 1.612485\n",
      "[Epoch 143/1000] Train Loss: 1.307698 | Val Loss: 1.602806\n",
      "[Epoch 144/1000] Train Loss: 1.156694 | Val Loss: 1.607865\n",
      "[Epoch 145/1000] Train Loss: 1.196303 | Val Loss: 1.593174\n",
      "[Epoch 146/1000] Train Loss: 1.228428 | Val Loss: 1.601913\n",
      "[Epoch 147/1000] Train Loss: 1.237338 | Val Loss: 1.604327\n",
      "[Epoch 148/1000] Train Loss: 1.408204 | Val Loss: 1.599012\n",
      "[Epoch 149/1000] Train Loss: 1.168616 | Val Loss: 1.595070\n",
      "[Epoch 150/1000] Train Loss: 1.284752 | Val Loss: 1.580251\n",
      "[Epoch 151/1000] Train Loss: 1.607219 | Val Loss: 1.591504\n",
      "[Epoch 152/1000] Train Loss: 1.464778 | Val Loss: 1.576038\n",
      "[Epoch 153/1000] Train Loss: 1.274569 | Val Loss: 1.567694\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 154/1000] Train Loss: 1.191912 | Val Loss: 1.578318\n",
      "[Epoch 155/1000] Train Loss: 1.145932 | Val Loss: 1.592475\n",
      "[Epoch 156/1000] Train Loss: 1.202293 | Val Loss: 1.571420\n",
      "[Epoch 157/1000] Train Loss: 1.156682 | Val Loss: 1.576270\n",
      "[Epoch 158/1000] Train Loss: 1.239650 | Val Loss: 1.588004\n",
      "[Epoch 159/1000] Train Loss: 1.189301 | Val Loss: 1.596848\n",
      "[Epoch 160/1000] Train Loss: 1.176484 | Val Loss: 1.576214\n",
      "[Epoch 161/1000] Train Loss: 1.175070 | Val Loss: 1.582225\n",
      "[Epoch 162/1000] Train Loss: 1.168369 | Val Loss: 1.586192\n",
      "[Epoch 163/1000] Train Loss: 1.194984 | Val Loss: 1.565751\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 164/1000] Train Loss: 1.170718 | Val Loss: 1.586887\n",
      "[Epoch 165/1000] Train Loss: 1.202039 | Val Loss: 1.589228\n",
      "[Epoch 166/1000] Train Loss: 1.480353 | Val Loss: 1.601116\n",
      "[Epoch 167/1000] Train Loss: 1.179631 | Val Loss: 1.598472\n",
      "[Epoch 168/1000] Train Loss: 1.563352 | Val Loss: 1.577437\n",
      "[Epoch 169/1000] Train Loss: 1.428417 | Val Loss: 1.582910\n",
      "[Epoch 170/1000] Train Loss: 1.217576 | Val Loss: 1.584748\n",
      "[Epoch 171/1000] Train Loss: 1.239024 | Val Loss: 1.584240\n",
      "[Epoch 172/1000] Train Loss: 1.144950 | Val Loss: 1.579101\n",
      "[Epoch 173/1000] Train Loss: 1.809939 | Val Loss: 1.584417\n",
      "[Epoch 174/1000] Train Loss: 1.333026 | Val Loss: 1.583666\n",
      "[Epoch 175/1000] Train Loss: 1.195195 | Val Loss: 1.583980\n",
      "[Epoch 176/1000] Train Loss: 1.213243 | Val Loss: 1.572983\n",
      "[Epoch 177/1000] Train Loss: 1.214024 | Val Loss: 1.581563\n",
      "[Epoch 178/1000] Train Loss: 1.248462 | Val Loss: 1.566771\n",
      "[Epoch 179/1000] Train Loss: 1.389841 | Val Loss: 1.564535\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 180/1000] Train Loss: 1.166508 | Val Loss: 1.569399\n",
      "[Epoch 181/1000] Train Loss: 1.162466 | Val Loss: 1.577416\n",
      "[Epoch 182/1000] Train Loss: 1.153583 | Val Loss: 1.572743\n",
      "[Epoch 183/1000] Train Loss: 1.153682 | Val Loss: 1.559117\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 184/1000] Train Loss: 1.357234 | Val Loss: 1.585372\n",
      "[Epoch 185/1000] Train Loss: 1.181246 | Val Loss: 1.576502\n",
      "[Epoch 186/1000] Train Loss: 1.274844 | Val Loss: 1.561369\n",
      "[Epoch 187/1000] Train Loss: 1.195155 | Val Loss: 1.565237\n",
      "[Epoch 188/1000] Train Loss: 1.192525 | Val Loss: 1.584291\n",
      "[Epoch 189/1000] Train Loss: 1.309685 | Val Loss: 1.583772\n",
      "[Epoch 190/1000] Train Loss: 1.225335 | Val Loss: 1.591256\n",
      "[Epoch 191/1000] Train Loss: 1.200608 | Val Loss: 1.591765\n",
      "[Epoch 192/1000] Train Loss: 1.455356 | Val Loss: 1.567894\n",
      "[Epoch 193/1000] Train Loss: 1.219120 | Val Loss: 1.574753\n",
      "[Epoch 194/1000] Train Loss: 1.172025 | Val Loss: 1.576814\n",
      "[Epoch 195/1000] Train Loss: 1.140927 | Val Loss: 1.557690\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 196/1000] Train Loss: 1.209156 | Val Loss: 1.571843\n",
      "[Epoch 197/1000] Train Loss: 1.685551 | Val Loss: 1.582113\n",
      "[Epoch 198/1000] Train Loss: 1.218267 | Val Loss: 1.581056\n",
      "[Epoch 199/1000] Train Loss: 1.256267 | Val Loss: 1.591963\n",
      "[Epoch 200/1000] Train Loss: 1.147102 | Val Loss: 1.564335\n",
      "[Epoch 201/1000] Train Loss: 1.295315 | Val Loss: 1.578986\n",
      "[Epoch 202/1000] Train Loss: 1.245836 | Val Loss: 1.586259\n",
      "[Epoch 203/1000] Train Loss: 1.158121 | Val Loss: 1.589753\n",
      "[Epoch 204/1000] Train Loss: 1.642602 | Val Loss: 1.591647\n",
      "[Epoch 205/1000] Train Loss: 1.170519 | Val Loss: 1.584244\n",
      "[Epoch 206/1000] Train Loss: 1.315137 | Val Loss: 1.590479\n",
      "[Epoch 207/1000] Train Loss: 1.201534 | Val Loss: 1.582776\n",
      "[Epoch 208/1000] Train Loss: 1.222728 | Val Loss: 1.584712\n",
      "[Epoch 209/1000] Train Loss: 1.390002 | Val Loss: 1.580794\n",
      "[Epoch 210/1000] Train Loss: 1.222242 | Val Loss: 1.585915\n",
      "[Epoch 211/1000] Train Loss: 1.490850 | Val Loss: 1.582118\n",
      "[Epoch 212/1000] Train Loss: 1.247284 | Val Loss: 1.578482\n",
      "[Epoch 213/1000] Train Loss: 1.413515 | Val Loss: 1.582097\n",
      "[Epoch 214/1000] Train Loss: 1.196861 | Val Loss: 1.585635\n",
      "[Epoch 215/1000] Train Loss: 1.757025 | Val Loss: 1.578741\n",
      "[Epoch 216/1000] Train Loss: 1.364220 | Val Loss: 1.577311\n",
      "[Epoch 217/1000] Train Loss: 1.192160 | Val Loss: 1.582396\n",
      "[Epoch 218/1000] Train Loss: 2.076534 | Val Loss: 1.588422\n",
      "[Epoch 219/1000] Train Loss: 1.177181 | Val Loss: 1.577670\n",
      "[Epoch 220/1000] Train Loss: 1.196117 | Val Loss: 1.572963\n",
      "[Epoch 221/1000] Train Loss: 1.254080 | Val Loss: 1.565879\n",
      "[Epoch 222/1000] Train Loss: 1.216169 | Val Loss: 1.564533\n",
      "[Epoch 223/1000] Train Loss: 1.181432 | Val Loss: 1.576896\n",
      "[Epoch 224/1000] Train Loss: 1.156977 | Val Loss: 1.573936\n",
      "[Epoch 225/1000] Train Loss: 1.274197 | Val Loss: 1.584567\n",
      "[Epoch 226/1000] Train Loss: 1.185094 | Val Loss: 1.576075\n",
      "[Epoch 227/1000] Train Loss: 1.454156 | Val Loss: 1.582335\n",
      "[Epoch 228/1000] Train Loss: 1.283924 | Val Loss: 1.573977\n",
      "[Epoch 229/1000] Train Loss: 1.966517 | Val Loss: 1.571177\n",
      "[Epoch 230/1000] Train Loss: 1.242846 | Val Loss: 1.554186\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 231/1000] Train Loss: 1.144046 | Val Loss: 1.573944\n",
      "[Epoch 232/1000] Train Loss: 1.251282 | Val Loss: 1.557944\n",
      "[Epoch 233/1000] Train Loss: 1.319988 | Val Loss: 1.566382\n",
      "[Epoch 234/1000] Train Loss: 1.475501 | Val Loss: 1.569462\n",
      "[Epoch 235/1000] Train Loss: 1.195062 | Val Loss: 1.571893\n",
      "[Epoch 236/1000] Train Loss: 1.154181 | Val Loss: 1.565033\n",
      "[Epoch 237/1000] Train Loss: 1.457934 | Val Loss: 1.576128\n",
      "[Epoch 238/1000] Train Loss: 1.356275 | Val Loss: 1.561534\n",
      "[Epoch 239/1000] Train Loss: 1.308859 | Val Loss: 1.566707\n",
      "[Epoch 240/1000] Train Loss: 1.193109 | Val Loss: 1.552462\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 241/1000] Train Loss: 1.546145 | Val Loss: 1.560697\n",
      "[Epoch 242/1000] Train Loss: 1.307002 | Val Loss: 1.550480\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 243/1000] Train Loss: 1.160608 | Val Loss: 1.558058\n",
      "[Epoch 244/1000] Train Loss: 1.172425 | Val Loss: 1.575736\n",
      "[Epoch 245/1000] Train Loss: 1.290093 | Val Loss: 1.572361\n",
      "[Epoch 246/1000] Train Loss: 1.189697 | Val Loss: 1.579249\n",
      "[Epoch 247/1000] Train Loss: 1.143524 | Val Loss: 1.573845\n",
      "[Epoch 248/1000] Train Loss: 1.192435 | Val Loss: 1.575553\n",
      "[Epoch 249/1000] Train Loss: 1.144341 | Val Loss: 1.555109\n",
      "[Epoch 250/1000] Train Loss: 1.213638 | Val Loss: 1.562220\n",
      "[Epoch 251/1000] Train Loss: 1.209216 | Val Loss: 1.561415\n",
      "[Epoch 252/1000] Train Loss: 1.638213 | Val Loss: 1.570562\n",
      "[Epoch 253/1000] Train Loss: 1.159497 | Val Loss: 1.548377\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 254/1000] Train Loss: 1.142607 | Val Loss: 1.552727\n",
      "[Epoch 255/1000] Train Loss: 1.162150 | Val Loss: 1.567572\n",
      "[Epoch 256/1000] Train Loss: 1.189237 | Val Loss: 1.567547\n",
      "[Epoch 257/1000] Train Loss: 1.414033 | Val Loss: 1.575797\n",
      "[Epoch 258/1000] Train Loss: 1.210069 | Val Loss: 1.577530\n",
      "[Epoch 259/1000] Train Loss: 1.383615 | Val Loss: 1.560727\n",
      "[Epoch 260/1000] Train Loss: 1.501060 | Val Loss: 1.555346\n",
      "[Epoch 261/1000] Train Loss: 1.143459 | Val Loss: 1.558119\n",
      "[Epoch 262/1000] Train Loss: 1.147881 | Val Loss: 1.554553\n",
      "[Epoch 263/1000] Train Loss: 1.132344 | Val Loss: 1.546075\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 264/1000] Train Loss: 1.350573 | Val Loss: 1.562501\n",
      "[Epoch 265/1000] Train Loss: 1.244111 | Val Loss: 1.566287\n",
      "[Epoch 266/1000] Train Loss: 1.273669 | Val Loss: 1.581220\n",
      "[Epoch 267/1000] Train Loss: 1.330080 | Val Loss: 1.578476\n",
      "[Epoch 268/1000] Train Loss: 1.230248 | Val Loss: 1.571261\n",
      "[Epoch 269/1000] Train Loss: 1.160579 | Val Loss: 1.577205\n",
      "[Epoch 270/1000] Train Loss: 1.138009 | Val Loss: 1.551826\n",
      "[Epoch 271/1000] Train Loss: 1.149865 | Val Loss: 1.555648\n",
      "[Epoch 272/1000] Train Loss: 1.199779 | Val Loss: 1.557400\n",
      "[Epoch 273/1000] Train Loss: 1.379475 | Val Loss: 1.547272\n",
      "[Epoch 274/1000] Train Loss: 1.245225 | Val Loss: 1.564831\n",
      "[Epoch 275/1000] Train Loss: 1.195741 | Val Loss: 1.571513\n",
      "[Epoch 276/1000] Train Loss: 1.158282 | Val Loss: 1.575789\n",
      "[Epoch 277/1000] Train Loss: 1.140681 | Val Loss: 1.578286\n",
      "[Epoch 278/1000] Train Loss: 1.153219 | Val Loss: 1.566873\n",
      "[Epoch 279/1000] Train Loss: 1.479939 | Val Loss: 1.573219\n",
      "[Epoch 280/1000] Train Loss: 1.207034 | Val Loss: 1.559885\n",
      "[Epoch 281/1000] Train Loss: 1.357616 | Val Loss: 1.562541\n",
      "[Epoch 282/1000] Train Loss: 1.386234 | Val Loss: 1.572717\n",
      "[Epoch 283/1000] Train Loss: 1.179493 | Val Loss: 1.552071\n",
      "[Epoch 284/1000] Train Loss: 1.349490 | Val Loss: 1.553885\n",
      "[Epoch 285/1000] Train Loss: 1.207208 | Val Loss: 1.555661\n",
      "[Epoch 286/1000] Train Loss: 1.323512 | Val Loss: 1.567320\n",
      "[Epoch 287/1000] Train Loss: 1.523974 | Val Loss: 1.574304\n",
      "[Epoch 288/1000] Train Loss: 1.208040 | Val Loss: 1.573524\n",
      "[Epoch 289/1000] Train Loss: 1.144465 | Val Loss: 1.559176\n",
      "[Epoch 290/1000] Train Loss: 1.166455 | Val Loss: 1.558826\n",
      "[Epoch 291/1000] Train Loss: 1.188320 | Val Loss: 1.559240\n",
      "[Epoch 292/1000] Train Loss: 1.304336 | Val Loss: 1.569633\n",
      "[Epoch 293/1000] Train Loss: 1.684813 | Val Loss: 1.567516\n",
      "[Epoch 294/1000] Train Loss: 1.201939 | Val Loss: 1.569670\n",
      "[Epoch 295/1000] Train Loss: 1.151027 | Val Loss: 1.550852\n",
      "[Epoch 296/1000] Train Loss: 1.309125 | Val Loss: 1.556117\n",
      "[Epoch 297/1000] Train Loss: 1.373001 | Val Loss: 1.552129\n",
      "[Epoch 298/1000] Train Loss: 1.345356 | Val Loss: 1.563742\n",
      "[Epoch 299/1000] Train Loss: 1.185418 | Val Loss: 1.570681\n",
      "[Epoch 300/1000] Train Loss: 1.189208 | Val Loss: 1.565121\n",
      "[Epoch 301/1000] Train Loss: 1.240989 | Val Loss: 1.553699\n",
      "[Epoch 302/1000] Train Loss: 1.160912 | Val Loss: 1.545500\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 303/1000] Train Loss: 1.370251 | Val Loss: 1.552020\n",
      "[Epoch 304/1000] Train Loss: 1.162190 | Val Loss: 1.542159\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 305/1000] Train Loss: 1.156243 | Val Loss: 1.540875\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 306/1000] Train Loss: 1.205425 | Val Loss: 1.559956\n",
      "[Epoch 307/1000] Train Loss: 1.318713 | Val Loss: 1.563879\n",
      "[Epoch 308/1000] Train Loss: 1.288167 | Val Loss: 1.568531\n",
      "[Epoch 309/1000] Train Loss: 1.171013 | Val Loss: 1.572949\n",
      "[Epoch 310/1000] Train Loss: 1.147020 | Val Loss: 1.570504\n",
      "[Epoch 311/1000] Train Loss: 1.180310 | Val Loss: 1.570280\n",
      "[Epoch 312/1000] Train Loss: 1.244647 | Val Loss: 1.573470\n",
      "[Epoch 313/1000] Train Loss: 1.163402 | Val Loss: 1.573592\n",
      "[Epoch 314/1000] Train Loss: 1.164283 | Val Loss: 1.557461\n",
      "[Epoch 315/1000] Train Loss: 1.335289 | Val Loss: 1.567023\n",
      "[Epoch 316/1000] Train Loss: 1.414411 | Val Loss: 1.551830\n",
      "[Epoch 317/1000] Train Loss: 1.268053 | Val Loss: 1.561394\n",
      "[Epoch 318/1000] Train Loss: 1.203263 | Val Loss: 1.568488\n",
      "[Epoch 319/1000] Train Loss: 1.296781 | Val Loss: 1.561854\n",
      "[Epoch 320/1000] Train Loss: 1.341219 | Val Loss: 1.549958\n",
      "[Epoch 321/1000] Train Loss: 1.142779 | Val Loss: 1.542577\n",
      "[Epoch 322/1000] Train Loss: 1.138887 | Val Loss: 1.547548\n",
      "[Epoch 323/1000] Train Loss: 1.142605 | Val Loss: 1.553226\n",
      "[Epoch 324/1000] Train Loss: 1.615030 | Val Loss: 1.562746\n",
      "[Epoch 325/1000] Train Loss: 1.190686 | Val Loss: 1.558891\n",
      "[Epoch 326/1000] Train Loss: 1.203170 | Val Loss: 1.557226\n",
      "[Epoch 327/1000] Train Loss: 1.179409 | Val Loss: 1.556454\n",
      "[Epoch 328/1000] Train Loss: 1.182276 | Val Loss: 1.554023\n",
      "[Epoch 329/1000] Train Loss: 1.166200 | Val Loss: 1.554910\n",
      "[Epoch 330/1000] Train Loss: 1.165971 | Val Loss: 1.554527\n",
      "[Epoch 331/1000] Train Loss: 1.360595 | Val Loss: 1.566956\n",
      "[Epoch 332/1000] Train Loss: 1.285632 | Val Loss: 1.546978\n",
      "[Epoch 333/1000] Train Loss: 1.166718 | Val Loss: 1.539824\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 334/1000] Train Loss: 1.390128 | Val Loss: 1.551169\n",
      "[Epoch 335/1000] Train Loss: 1.184450 | Val Loss: 1.553444\n",
      "[Epoch 336/1000] Train Loss: 1.427388 | Val Loss: 1.557054\n",
      "[Epoch 337/1000] Train Loss: 1.340713 | Val Loss: 1.563054\n",
      "[Epoch 338/1000] Train Loss: 1.184148 | Val Loss: 1.558839\n",
      "[Epoch 339/1000] Train Loss: 1.132091 | Val Loss: 1.557501\n",
      "[Epoch 340/1000] Train Loss: 1.239614 | Val Loss: 1.565667\n",
      "[Epoch 341/1000] Train Loss: 1.240817 | Val Loss: 1.547945\n",
      "[Epoch 342/1000] Train Loss: 1.144405 | Val Loss: 1.543823\n",
      "[Epoch 343/1000] Train Loss: 1.228879 | Val Loss: 1.554178\n",
      "[Epoch 344/1000] Train Loss: 1.289655 | Val Loss: 1.562775\n",
      "[Epoch 345/1000] Train Loss: 1.119496 | Val Loss: 1.541939\n",
      "[Epoch 346/1000] Train Loss: 1.152766 | Val Loss: 1.559471\n",
      "[Epoch 347/1000] Train Loss: 1.255658 | Val Loss: 1.571304\n",
      "[Epoch 348/1000] Train Loss: 1.141516 | Val Loss: 1.554145\n",
      "[Epoch 349/1000] Train Loss: 1.212424 | Val Loss: 1.541125\n",
      "[Epoch 350/1000] Train Loss: 1.182660 | Val Loss: 1.559827\n",
      "[Epoch 351/1000] Train Loss: 1.134921 | Val Loss: 1.549705\n",
      "[Epoch 352/1000] Train Loss: 1.238884 | Val Loss: 1.565949\n",
      "[Epoch 353/1000] Train Loss: 1.282049 | Val Loss: 1.546896\n",
      "[Epoch 354/1000] Train Loss: 1.189042 | Val Loss: 1.549891\n",
      "[Epoch 355/1000] Train Loss: 1.291289 | Val Loss: 1.550372\n",
      "[Epoch 356/1000] Train Loss: 1.166072 | Val Loss: 1.561344\n",
      "[Epoch 357/1000] Train Loss: 1.212816 | Val Loss: 1.560512\n",
      "[Epoch 358/1000] Train Loss: 1.298736 | Val Loss: 1.560248\n",
      "[Epoch 359/1000] Train Loss: 1.178901 | Val Loss: 1.559251\n",
      "[Epoch 360/1000] Train Loss: 1.251498 | Val Loss: 1.541421\n",
      "[Epoch 361/1000] Train Loss: 1.310299 | Val Loss: 1.554001\n",
      "[Epoch 362/1000] Train Loss: 1.141517 | Val Loss: 1.551421\n",
      "[Epoch 363/1000] Train Loss: 1.203122 | Val Loss: 1.552594\n",
      "[Epoch 364/1000] Train Loss: 1.142411 | Val Loss: 1.560734\n",
      "[Epoch 365/1000] Train Loss: 1.141533 | Val Loss: 1.548602\n",
      "[Epoch 366/1000] Train Loss: 1.180419 | Val Loss: 1.549693\n",
      "[Epoch 367/1000] Train Loss: 1.227736 | Val Loss: 1.540005\n",
      "[Epoch 368/1000] Train Loss: 1.146765 | Val Loss: 1.538304\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 369/1000] Train Loss: 1.234337 | Val Loss: 1.549439\n",
      "[Epoch 370/1000] Train Loss: 1.192296 | Val Loss: 1.554154\n",
      "[Epoch 371/1000] Train Loss: 1.267809 | Val Loss: 1.550910\n",
      "[Epoch 372/1000] Train Loss: 1.154407 | Val Loss: 1.551548\n",
      "[Epoch 373/1000] Train Loss: 1.200523 | Val Loss: 1.553107\n",
      "[Epoch 374/1000] Train Loss: 1.153791 | Val Loss: 1.552999\n",
      "[Epoch 375/1000] Train Loss: 1.260401 | Val Loss: 1.554582\n",
      "[Epoch 376/1000] Train Loss: 1.253871 | Val Loss: 1.558796\n",
      "[Epoch 377/1000] Train Loss: 1.160116 | Val Loss: 1.547013\n",
      "[Epoch 378/1000] Train Loss: 1.161527 | Val Loss: 1.559058\n",
      "[Epoch 379/1000] Train Loss: 1.210878 | Val Loss: 1.555968\n",
      "[Epoch 380/1000] Train Loss: 1.285386 | Val Loss: 1.561539\n",
      "[Epoch 381/1000] Train Loss: 1.157182 | Val Loss: 1.564678\n",
      "[Epoch 382/1000] Train Loss: 1.124432 | Val Loss: 1.543352\n",
      "[Epoch 383/1000] Train Loss: 1.194240 | Val Loss: 1.558242\n",
      "[Epoch 384/1000] Train Loss: 1.189803 | Val Loss: 1.565783\n",
      "[Epoch 385/1000] Train Loss: 1.131404 | Val Loss: 1.555809\n",
      "[Epoch 386/1000] Train Loss: 1.159848 | Val Loss: 1.553697\n",
      "[Epoch 387/1000] Train Loss: 1.357376 | Val Loss: 1.565135\n",
      "[Epoch 388/1000] Train Loss: 1.558461 | Val Loss: 1.558460\n",
      "[Epoch 389/1000] Train Loss: 1.300775 | Val Loss: 1.556485\n",
      "[Epoch 390/1000] Train Loss: 1.217587 | Val Loss: 1.563005\n",
      "[Epoch 391/1000] Train Loss: 1.187697 | Val Loss: 1.556544\n",
      "[Epoch 392/1000] Train Loss: 1.222908 | Val Loss: 1.562666\n",
      "[Epoch 393/1000] Train Loss: 1.149462 | Val Loss: 1.563451\n",
      "[Epoch 394/1000] Train Loss: 1.149310 | Val Loss: 1.557981\n",
      "[Epoch 395/1000] Train Loss: 1.192853 | Val Loss: 1.560145\n",
      "[Epoch 396/1000] Train Loss: 1.352524 | Val Loss: 1.550901\n",
      "[Epoch 397/1000] Train Loss: 1.115183 | Val Loss: 1.542715\n",
      "[Epoch 398/1000] Train Loss: 1.195925 | Val Loss: 1.548507\n",
      "[Epoch 399/1000] Train Loss: 1.143266 | Val Loss: 1.550423\n",
      "[Epoch 400/1000] Train Loss: 1.396989 | Val Loss: 1.555418\n",
      "[Epoch 401/1000] Train Loss: 1.131000 | Val Loss: 1.552087\n",
      "[Epoch 402/1000] Train Loss: 1.246891 | Val Loss: 1.544783\n",
      "[Epoch 403/1000] Train Loss: 1.511794 | Val Loss: 1.558670\n",
      "[Epoch 404/1000] Train Loss: 1.590949 | Val Loss: 1.568557\n",
      "[Epoch 405/1000] Train Loss: 1.214097 | Val Loss: 1.562655\n",
      "[Epoch 406/1000] Train Loss: 1.126822 | Val Loss: 1.548695\n",
      "[Epoch 407/1000] Train Loss: 1.201486 | Val Loss: 1.551723\n",
      "[Epoch 408/1000] Train Loss: 1.214528 | Val Loss: 1.560742\n",
      "[Epoch 409/1000] Train Loss: 1.259265 | Val Loss: 1.567857\n",
      "[Epoch 410/1000] Train Loss: 1.223174 | Val Loss: 1.564503\n",
      "[Epoch 411/1000] Train Loss: 1.207246 | Val Loss: 1.563344\n",
      "[Epoch 412/1000] Train Loss: 1.150674 | Val Loss: 1.544934\n",
      "[Epoch 413/1000] Train Loss: 1.176233 | Val Loss: 1.540570\n",
      "[Epoch 414/1000] Train Loss: 1.398687 | Val Loss: 1.557545\n",
      "[Epoch 415/1000] Train Loss: 1.124006 | Val Loss: 1.551328\n",
      "[Epoch 416/1000] Train Loss: 1.510947 | Val Loss: 1.549548\n",
      "[Epoch 417/1000] Train Loss: 1.245969 | Val Loss: 1.562323\n",
      "[Epoch 418/1000] Train Loss: 1.198942 | Val Loss: 1.552308\n",
      "[Epoch 419/1000] Train Loss: 1.235678 | Val Loss: 1.562820\n",
      "[Epoch 420/1000] Train Loss: 1.134379 | Val Loss: 1.562413\n",
      "[Epoch 421/1000] Train Loss: 1.165496 | Val Loss: 1.565673\n",
      "[Epoch 422/1000] Train Loss: 1.159454 | Val Loss: 1.562767\n",
      "[Epoch 423/1000] Train Loss: 1.219101 | Val Loss: 1.558421\n",
      "[Epoch 424/1000] Train Loss: 1.227536 | Val Loss: 1.556330\n",
      "[Epoch 425/1000] Train Loss: 1.183571 | Val Loss: 1.557033\n",
      "[Epoch 426/1000] Train Loss: 1.360994 | Val Loss: 1.555139\n",
      "[Epoch 427/1000] Train Loss: 1.353667 | Val Loss: 1.546612\n",
      "[Epoch 428/1000] Train Loss: 1.302723 | Val Loss: 1.546641\n",
      "[Epoch 429/1000] Train Loss: 1.164853 | Val Loss: 1.541382\n",
      "[Epoch 430/1000] Train Loss: 1.689430 | Val Loss: 1.546626\n",
      "[Epoch 431/1000] Train Loss: 1.184775 | Val Loss: 1.561364\n",
      "[Epoch 432/1000] Train Loss: 1.119613 | Val Loss: 1.547002\n",
      "[Epoch 433/1000] Train Loss: 1.264981 | Val Loss: 1.550590\n",
      "[Epoch 434/1000] Train Loss: 1.335928 | Val Loss: 1.552123\n",
      "[Epoch 435/1000] Train Loss: 1.151768 | Val Loss: 1.547411\n",
      "[Epoch 436/1000] Train Loss: 1.186011 | Val Loss: 1.545573\n",
      "[Epoch 437/1000] Train Loss: 1.210511 | Val Loss: 1.552490\n",
      "[Epoch 438/1000] Train Loss: 1.143509 | Val Loss: 1.543848\n",
      "[Epoch 439/1000] Train Loss: 1.315783 | Val Loss: 1.560361\n",
      "[Epoch 440/1000] Train Loss: 1.487100 | Val Loss: 1.548993\n",
      "[Epoch 441/1000] Train Loss: 1.210994 | Val Loss: 1.541884\n",
      "[Epoch 442/1000] Train Loss: 1.136538 | Val Loss: 1.533801\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 443/1000] Train Loss: 1.323996 | Val Loss: 1.545643\n",
      "[Epoch 444/1000] Train Loss: 1.407644 | Val Loss: 1.540077\n",
      "[Epoch 445/1000] Train Loss: 1.166936 | Val Loss: 1.538378\n",
      "[Epoch 446/1000] Train Loss: 1.157683 | Val Loss: 1.536564\n",
      "[Epoch 447/1000] Train Loss: 1.414023 | Val Loss: 1.543610\n",
      "[Epoch 448/1000] Train Loss: 1.180887 | Val Loss: 1.553927\n",
      "[Epoch 449/1000] Train Loss: 1.666636 | Val Loss: 1.544439\n",
      "[Epoch 450/1000] Train Loss: 1.157509 | Val Loss: 1.538303\n",
      "[Epoch 451/1000] Train Loss: 1.204411 | Val Loss: 1.540355\n",
      "[Epoch 452/1000] Train Loss: 1.233606 | Val Loss: 1.548565\n",
      "[Epoch 453/1000] Train Loss: 1.143536 | Val Loss: 1.554444\n",
      "[Epoch 454/1000] Train Loss: 1.150446 | Val Loss: 1.554144\n",
      "[Epoch 455/1000] Train Loss: 1.189158 | Val Loss: 1.554057\n",
      "[Epoch 456/1000] Train Loss: 1.157033 | Val Loss: 1.560179\n",
      "[Epoch 457/1000] Train Loss: 1.576637 | Val Loss: 1.557665\n",
      "[Epoch 458/1000] Train Loss: 1.162396 | Val Loss: 1.537339\n",
      "[Epoch 459/1000] Train Loss: 1.518132 | Val Loss: 1.546926\n",
      "[Epoch 460/1000] Train Loss: 1.134750 | Val Loss: 1.545081\n",
      "[Epoch 461/1000] Train Loss: 1.150977 | Val Loss: 1.556162\n",
      "[Epoch 462/1000] Train Loss: 1.186048 | Val Loss: 1.547381\n",
      "[Epoch 463/1000] Train Loss: 1.406613 | Val Loss: 1.558488\n",
      "[Epoch 464/1000] Train Loss: 1.133956 | Val Loss: 1.552066\n",
      "[Epoch 465/1000] Train Loss: 1.152291 | Val Loss: 1.548129\n",
      "[Epoch 466/1000] Train Loss: 1.179113 | Val Loss: 1.541484\n",
      "[Epoch 467/1000] Train Loss: 1.144266 | Val Loss: 1.539339\n",
      "[Epoch 468/1000] Train Loss: 1.162897 | Val Loss: 1.550902\n",
      "[Epoch 469/1000] Train Loss: 1.471251 | Val Loss: 1.541384\n",
      "[Epoch 470/1000] Train Loss: 1.222021 | Val Loss: 1.543533\n",
      "[Epoch 471/1000] Train Loss: 1.163366 | Val Loss: 1.540236\n",
      "[Epoch 472/1000] Train Loss: 1.357088 | Val Loss: 1.541436\n",
      "[Epoch 473/1000] Train Loss: 1.376096 | Val Loss: 1.557403\n",
      "[Epoch 474/1000] Train Loss: 1.150148 | Val Loss: 1.547169\n",
      "[Epoch 475/1000] Train Loss: 1.209731 | Val Loss: 1.554056\n",
      "[Epoch 476/1000] Train Loss: 1.262963 | Val Loss: 1.550757\n",
      "[Epoch 477/1000] Train Loss: 1.263739 | Val Loss: 1.558824\n",
      "[Epoch 478/1000] Train Loss: 1.416456 | Val Loss: 1.552205\n",
      "[Epoch 479/1000] Train Loss: 1.157760 | Val Loss: 1.543789\n",
      "[Epoch 480/1000] Train Loss: 1.230710 | Val Loss: 1.546440\n",
      "[Epoch 481/1000] Train Loss: 1.192823 | Val Loss: 1.534441\n",
      "[Epoch 482/1000] Train Loss: 1.397708 | Val Loss: 1.542817\n",
      "[Epoch 483/1000] Train Loss: 1.250619 | Val Loss: 1.548356\n",
      "[Epoch 484/1000] Train Loss: 1.131641 | Val Loss: 1.538721\n",
      "[Epoch 485/1000] Train Loss: 1.118476 | Val Loss: 1.538446\n",
      "[Epoch 486/1000] Train Loss: 1.166979 | Val Loss: 1.548467\n",
      "[Epoch 487/1000] Train Loss: 1.128269 | Val Loss: 1.541573\n",
      "[Epoch 488/1000] Train Loss: 1.252022 | Val Loss: 1.532467\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 489/1000] Train Loss: 1.298483 | Val Loss: 1.546034\n",
      "[Epoch 490/1000] Train Loss: 1.222032 | Val Loss: 1.552142\n",
      "[Epoch 491/1000] Train Loss: 1.290730 | Val Loss: 1.543573\n",
      "[Epoch 492/1000] Train Loss: 1.129654 | Val Loss: 1.543952\n",
      "[Epoch 493/1000] Train Loss: 1.136306 | Val Loss: 1.536466\n",
      "[Epoch 494/1000] Train Loss: 1.144277 | Val Loss: 1.532016\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 495/1000] Train Loss: 1.158731 | Val Loss: 1.530747\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 496/1000] Train Loss: 1.277729 | Val Loss: 1.549829\n",
      "[Epoch 497/1000] Train Loss: 1.275335 | Val Loss: 1.550847\n",
      "[Epoch 498/1000] Train Loss: 1.256467 | Val Loss: 1.552665\n",
      "[Epoch 499/1000] Train Loss: 1.130034 | Val Loss: 1.540368\n",
      "[Epoch 500/1000] Train Loss: 1.170134 | Val Loss: 1.531543\n",
      "[Epoch 501/1000] Train Loss: 1.332614 | Val Loss: 1.540251\n",
      "[Epoch 502/1000] Train Loss: 1.153584 | Val Loss: 1.540421\n",
      "[Epoch 503/1000] Train Loss: 1.129736 | Val Loss: 1.543368\n",
      "[Epoch 504/1000] Train Loss: 1.342964 | Val Loss: 1.533452\n",
      "[Epoch 505/1000] Train Loss: 1.232439 | Val Loss: 1.550307\n",
      "[Epoch 506/1000] Train Loss: 1.178277 | Val Loss: 1.549341\n",
      "[Epoch 507/1000] Train Loss: 1.255271 | Val Loss: 1.553525\n",
      "[Epoch 508/1000] Train Loss: 1.285812 | Val Loss: 1.561284\n",
      "[Epoch 509/1000] Train Loss: 1.185173 | Val Loss: 1.548978\n",
      "[Epoch 510/1000] Train Loss: 1.163329 | Val Loss: 1.551123\n",
      "[Epoch 511/1000] Train Loss: 1.247580 | Val Loss: 1.545792\n",
      "[Epoch 512/1000] Train Loss: 1.152222 | Val Loss: 1.543548\n",
      "[Epoch 513/1000] Train Loss: 1.131993 | Val Loss: 1.530080\n",
      "✅ Saved new best model at model_best.pt\n",
      "[Epoch 514/1000] Train Loss: 1.218793 | Val Loss: 1.538502\n",
      "[Epoch 515/1000] Train Loss: 1.232584 | Val Loss: 1.550023\n",
      "[Epoch 516/1000] Train Loss: 1.269933 | Val Loss: 1.552662\n",
      "[Epoch 517/1000] Train Loss: 1.122859 | Val Loss: 1.537821\n",
      "[Epoch 518/1000] Train Loss: 1.306185 | Val Loss: 1.540978\n",
      "[Epoch 519/1000] Train Loss: 1.408119 | Val Loss: 1.553300\n",
      "[Epoch 520/1000] Train Loss: 1.412469 | Val Loss: 1.552263\n",
      "[Epoch 521/1000] Train Loss: 1.290276 | Val Loss: 1.548789\n",
      "[Epoch 522/1000] Train Loss: 1.198809 | Val Loss: 1.552681\n",
      "[Epoch 523/1000] Train Loss: 1.351780 | Val Loss: 1.557413\n",
      "[Epoch 524/1000] Train Loss: 1.149694 | Val Loss: 1.550383\n",
      "[Epoch 525/1000] Train Loss: 1.115546 | Val Loss: 1.540648\n",
      "[Epoch 526/1000] Train Loss: 1.256139 | Val Loss: 1.550303\n",
      "[Epoch 527/1000] Train Loss: 1.136159 | Val Loss: 1.555663\n",
      "[Epoch 528/1000] Train Loss: 1.195532 | Val Loss: 1.551060\n",
      "[Epoch 529/1000] Train Loss: 1.164974 | Val Loss: 1.554185\n",
      "[Epoch 530/1000] Train Loss: 1.157314 | Val Loss: 1.559242\n",
      "[Epoch 531/1000] Train Loss: 1.321189 | Val Loss: 1.564183\n",
      "[Epoch 532/1000] Train Loss: 1.293981 | Val Loss: 1.554825\n",
      "[Epoch 533/1000] Train Loss: 1.137879 | Val Loss: 1.557818\n",
      "[Epoch 534/1000] Train Loss: 1.653741 | Val Loss: 1.564901\n",
      "[Epoch 535/1000] Train Loss: 1.192243 | Val Loss: 1.559135\n",
      "[Epoch 536/1000] Train Loss: 1.667534 | Val Loss: 1.548880\n",
      "[Epoch 537/1000] Train Loss: 1.385657 | Val Loss: 1.546985\n",
      "[Epoch 538/1000] Train Loss: 1.188329 | Val Loss: 1.539185\n",
      "[Epoch 539/1000] Train Loss: 1.331139 | Val Loss: 1.542387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m best_val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(CFG[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[1;32m----> 5\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m evaluate(model, val_loader)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCFG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Val Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[7], line 71\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, loader, optimizer)\u001B[0m\n\u001B[0;32m     69\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[0;32m     70\u001B[0m loss \u001B[38;5;241m=\u001B[39m masked_huber_loss(targets, outputs)\n\u001B[1;32m---> 71\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     72\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     73\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32md:\\Anacoda3\\envs\\pystack39_torch\\lib\\site-packages\\torch\\_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    647\u001B[0m     )\n\u001B[1;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\Anacoda3\\envs\\pystack39_torch\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\Anacoda3\\envs\\pystack39_torch\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    825\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    826\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# === 主训练循环 ===\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(CFG[\"n_epochs\"]):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{CFG['n_epochs']}] Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "    # 保存最优模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), CFG[\"model_save_path\"])\n",
    "        print(f\"✅ Saved new best model at {CFG['model_save_path']}\")\n",
    "\n",
    "print(\"🎉 Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystack39_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
